{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd868dbd-83d5-4cb8-9ff7-a241989bad83",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style='background:lightgreen'> Data-driven Astronomy  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f0002-3a7f-46bf-ac42-ff823c80b1bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4b – Week 4: Combining SQL and Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81445e85-47ff-43c2-8fe7-176136ece9e7",
   "metadata": {},
   "source": [
    "#### Taking it all in\n",
    "\n",
    "##### 1. To get started with basic Psycopg2 usage, write a function called select_all which queries either our Star or Planet table in PostgreSQL and returns all the rows using the following query: SELECT * FROM Star;\n",
    "\n",
    "##### Your function should take the name of the table as a string argument, so you can call it like to access the Star table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7fe5a-d17e-4763-9300-639595a7ff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "def select_all(a):\n",
    "    conn = psycopg2.connect(dbname='db', user='grok')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute('SELECT * FROM '+a+';')\n",
    "    return cursor.fetchall()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798d0709-a391-4cdf-bc00-cd0b1240bf71",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e03d99b-4d4b-4de5-a55e-9b50a9d20c6b",
   "metadata": {},
   "source": [
    "#### A proper median\n",
    "\n",
    "##### 2. Write a function called column_stats which calculates the mean and median of a selected column in either Star or Planet table. For this, let your function take two string arguments:\n",
    "\n",
    ">##### the name of the table;\n",
    ">##### the name of the column.\n",
    "##### and have it return the mean and median (in this order) of the selected column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01819fe8-d400-4738-89e8-f72b1f9dad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "\n",
    "conn = psycopg2.connect(dbname='db', user='grok')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def column_stats(a,b):\n",
    "    cursor.execute('SELECT '+b+' FROM '+a+';')\n",
    "    records = cursor.fetchall()\n",
    "    array = np.array(records)\n",
    "    return (np.mean(array),np.median(array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79a3565-3dbb-457d-88a2-a7a2ad3afefb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebf32d-5750-47cd-9b4e-aecbc10d0197",
   "metadata": {},
   "source": [
    "##### 3. Your first task is to replicate the following SQL query: \n",
    "\n",
    ">##### SELECT kepler_id, radius\n",
    ">##### FROM Star\n",
    ">##### WHERE radius > 1.0;\n",
    "\n",
    "##### The data is stored in stars.csv, with the kepler_id in the first column and the radius in the last.\n",
    "\n",
    "##### Write a function called query which takes the CSV filename as an argument and returns the data in a 2-column NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131829d-d341-49bd-b442-1dcdde5cc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query function here\n",
    "import numpy as np \n",
    "def query(a):\n",
    "    x=[]\n",
    "    b = np.loadtxt(a, delimiter=',')\n",
    "    for n in range (len(b)):\n",
    "        if b[n,2]>1.0:\n",
    "            x=x+[(b[n,0],b[n,2])]\n",
    "    return np.array(x)\n",
    "\n",
    "\n",
    "# You can use this to test your code\n",
    "# Everything inside this if-statement will be ignored by the automarker\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Compare your function output to the SQL query\n",
    "    result = query('stars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb5c06d-57e6-4a9e-bb1e-7798e704cd1b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8774e5f4-76e2-45b1-880a-182f1d35831d",
   "metadata": {},
   "source": [
    "#### Simple queries in Python 2/3\n",
    "\n",
    "##### 4. Let's add another element to our query. Sort the resulting table in ascending order to match the result you would get with:\n",
    "\n",
    ">##### SELECT kepler_id, radius\n",
    ">##### FROM Star\n",
    ">##### WHERE radius > 1.0\n",
    ">##### ORDER BY radius ASC;\n",
    "\n",
    "##### You can use your results from the last problem and then build up on that. Again, the function should be named query and it should take the filename as argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bce8f0e-1632-4a14-aeb3-e2aa22181e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query function here\n",
    "import numpy as np \n",
    "def query(a):\n",
    "    x=[]\n",
    "    b = np.loadtxt(a, delimiter=',')\n",
    "    for n in range (len(b)):\n",
    "        if b[n,2]>1.0:\n",
    "            x=x+[(b[n,0],b[n,2])]\n",
    "    d=[]\n",
    "    for n in range (len(x)):\n",
    "        d=d+[x[n][1]]\n",
    "    d= np.argsort(np.array(d))\n",
    "    return np.array(x)[d]\n",
    "\n",
    "\n",
    "# You can use this to test your code\n",
    "# Everything inside this if-statement will be ignored by the automarker\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Compare your function output to the SQL query\n",
    "    result = query('stars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367247e9-ef32-46fd-a2f3-afb89a1aee7c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3b10f-738d-4ce8-a612-84c5494f0412",
   "metadata": {},
   "source": [
    "#### Simple queries in Python 3/3\n",
    "\n",
    "##### 5. Let's add yet another element to our query. Join the Star table with the Planet table and calculate the size ratio, i.e. planet radius / star radius for each star-planet pair. Your query function should produce the same result as the SQL query:\n",
    "\n",
    ">##### SELECT p.radius/s.radius AS radius_ratio\n",
    ">##### FROM Planet AS p\n",
    ">##### INNER JOIN star AS s USING (kepler_id)\n",
    ">##### WHERE s.radius > 1.0\n",
    ">##### ORDER BY p.radius/s.radius ASC;\n",
    "\n",
    "##### You can use your results from the last problem and then build up on that. The function must be named query, but this time it should take two filenames as arguments, for the stars and planets.\n",
    "\n",
    "##### In planets.csv, the first column is the kepler_id and the second last column is the radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51565b-85e1-4a64-bded-838f2320e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your query function here\n",
    "import numpy as np \n",
    "def query(s,p):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    s = np.loadtxt(s, delimiter=',')\n",
    "    for n in range (len(s)):\n",
    "        if s[n,2]>1.0:\n",
    "            x=x+[(s[n,0],s[n,2])]\n",
    "    x=np.array(x)\n",
    "    p = np.loadtxt(p, delimiter=',',usecols=(0,5))\n",
    "    for n in range (len(p)):\n",
    "        for m in range (len(x)):\n",
    "            if x[m,0]==p[n,0]:\n",
    "                y=y+[[(p[n,1]/x[m,1])]]\n",
    "    y=np.array(y)\n",
    "    return np.sort(y[0:], axis = 0)\n",
    "\n",
    "\n",
    "# You can use this to test your code\n",
    "# Everything inside this if-statement will be ignored by the automarker\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Compare your function output to the SQL query\n",
    "    result = query('stars.csv', 'planets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c91600-5cb8-411d-b299-880ef3344b0b",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5513f1f-e82a-41b7-83f8-b3c1d9028570",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5a – Week 5: Building a regression classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3401290b-0f6a-4d1a-94ec-6fc164dabd4d",
   "metadata": {},
   "source": [
    "#### Features and Targets\n",
    "\n",
    "##### 6. Write a get_features_targets function that splits the training data into input features and their corresponding targets. In our case, the inputs are the 4 colour indices and our targets are the corresponding redshifts.\n",
    "\n",
    "##### Your function should return a tuple of:\n",
    "##### features: a NumPy array of dimensions m ⨉ 4, where m is the number of galaxies;\n",
    "##### targets: a 1D NumPy array of length m, containing the redshift for each galaxy.\n",
    "\n",
    "##### The data argument will be the structured array described on the previous slide. The u flux magnitudes and redshifts can be accessed as a column with data['u'] and data['redshift'].\n",
    "\n",
    "##### The four features are the colours u - g, g - r, r - i and i - z. To calculate the first column of features, subtract the u and g column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6ae29-b8ef-4fc3-940c-3dbc4361b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_features_targets(data):\n",
    "    # complete this function\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # load the data\n",
    "    data = np.load('sdss_galaxy_colors.npy')\n",
    "    \n",
    "    # call our function \n",
    "    features, targets = get_features_targets(data)\n",
    "    \n",
    "    # print the shape of the returned arrays\n",
    "    print(features[:2])\n",
    "    print(targets[:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c51682-c7f0-4138-b851-a7cb0bdb0bcb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23937a7-d2dd-4733-b50a-429127ea1f62",
   "metadata": {},
   "source": [
    "#### Decision Tree Regressor \n",
    "\n",
    "##### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f5beb-f696-493a-b78e-ca82ef3e34ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_features_targets(data):\n",
    "    # complete this function\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "\n",
    "# load the data and generate the features and targets\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "  \n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "dtr.fit(features, targets)\n",
    "\n",
    "predictions = dtr.predict(features)\n",
    "\n",
    "# print out the first 4 predicted redshifts\n",
    "print(predictions[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10987a9d-4685-49d8-9525-d01b5232b033",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5890870-8e36-4ea8-8569-f8d5b73f4f28",
   "metadata": {},
   "source": [
    "#### Calculating the median difference\n",
    "\n",
    "##### 8. In this problem we will implement the function median_diff. The function should calculate the median residual error of our model, i.e. the median difference between our predicted and actual redshifts.\n",
    "\n",
    "##### The median_diff function takes two arguments – the predicted and actual/target values. When we use this function later in the tutorial, these will corresponding to the predicted redshifts from our decision tree and their corresponding measured/target values.\n",
    "\n",
    "##### The median of differences should be calculated according to the formula:\n",
    "\n",
    "\n",
    "\n",
    "## Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5453fb-4a6e-411d-959e-ae6d1fed5f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# write a function that calculates the median of the differences\n",
    "# between our predicted and actual values\n",
    "\n",
    "def median_diff(predicted, actual):\n",
    "    return np.median(abs(predicted-actual))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load testing data\n",
    "    targets = np.load('targets.npy')\n",
    "    predictions = np.load('predictions.npy')\n",
    "\n",
    "    # call your function to measure the accuracy of the predictions\n",
    "    diff = median_diff(predictions, targets)\n",
    "\n",
    "    # print the median difference\n",
    "    print(\"Median difference: {:0.3f}\".format(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a2a091-4f25-488a-b196-4aa7fb77f192",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb877606-db0d-4b71-a636-608777092e6d",
   "metadata": {},
   "source": [
    "#### Validating our Model\n",
    "\n",
    "##### 9. In this problem, we will use median_diff from the previous question to validate the decision tree model. Your task is to complete the validate_model function.The function should split the features and targets into train and test subsets\n",
    "\n",
    "##### Your function should then use the training split (train_features and train_targets) to train the model.\n",
    "\n",
    "##### Finally, it should measure the accuracy of the model using median_diff on the test_targets and the predicted redshifts from test_features.\n",
    "\n",
    "##### The function should take 3 arguments:\n",
    "\n",
    "> #####  model: the decision tree regressor;\n",
    "> ##### features - the features for the data set;\n",
    "> ##### targets - The targets for the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56eba3ca-8b69-4cb1-a23a-ca2ec8a5bfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_features_targets(data):\n",
    "    # complete this function\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "def median_diff(predicted, actual):\n",
    "    return np.median(abs(predicted-actual))\n",
    "\n",
    "# write a function that splits the data into training and testing subsets\n",
    "\n",
    "def split(features,targets):\n",
    "    split = features.shape[0]//2\n",
    "    return features[:split],features[split:],targets[:split],targets[split:]\n",
    "  \n",
    "# trains the model and returns the prediction accuracy with median_diff\n",
    "\n",
    "def validate_model(model, features, targets):\n",
    "    train_features,test_features,train_targets,test_targets =split(features,targets)\n",
    "    dtr = DecisionTreeRegressor()\n",
    "    dtr.fit(train_features, train_targets)\n",
    "    predictions = dtr.predict(test_features)\n",
    "\n",
    "    # train the model\n",
    "\n",
    "    # get the predicted_redshifts\n",
    "  \n",
    "    # use median_diff function to calculate the accuracy\n",
    "    return median_diff(test_targets, predictions)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('sdss_galaxy_colors.npy')\n",
    "    features, targets = get_features_targets(data)\n",
    "\n",
    "    # initialize model\n",
    "    dtr = DecisionTreeRegressor()\n",
    "\n",
    "    # validate the model and print the med_diff\n",
    "    diff = validate_model(dtr, features, targets)\n",
    "    print('Median difference: {:f}'.format(diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7231e573-8963-4cf1-ba0c-a27c1d351ac1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c1905-ea17-423c-a9d5-1885c8c8b30c",
   "metadata": {},
   "source": [
    "#### Colour-Colour Redshift Plot \n",
    "\n",
    "##### 10. Your task here is simply to try and re-create the following plot.\n",
    "\n",
    "##### You should use the pyplot module of matplotlib which has already been imported and can be accessed through plt. In particular you can use the plt.scatter() function, with additional arguments s, c and cmap.\n",
    "\n",
    "##### We are interested in the u-g and r-i colour indices.\n",
    "\n",
    "##### You can make use of the plt.colorbar() function to show your scatter plots colour argument(c) to a colour bar on the side of the plot.\n",
    "\n",
    "##### Make sure you implement x and y labels and give your plot a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae87fc-5a27-42a6-8ab7-038f2ff8e991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Complete the following to make the plot\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('sdss_galaxy_colors.npy')\n",
    "    # Get a colour map\n",
    "    cmap = plt.get_cmap('YlOrRd')\n",
    "\n",
    "    # Define our colour indexes u-g and r-i\n",
    "    u,r=data['u']-data['g'],data['r']-data['i']\n",
    "\n",
    "    # Make a redshift array\n",
    "    redshift=data['redshift']\n",
    "\n",
    "    # Create the plot with plt.scatter and plt.colorbar\n",
    "    plot=plt.scatter(u,r,s=2,lw=0,c=redshift)\n",
    "    cb=plt.colorbar(plot)\n",
    "    cb.set_label('Redshift')\n",
    "\n",
    "    # Define your axis labels and plot title\n",
    "    plt.title('Redshift (colour) u-g versus r-i')\n",
    "    plt.xlabel('Colour index u-g')\n",
    "    plt.ylabel('Colour index r-i')\n",
    "\n",
    "    # Set any axis limits\n",
    "    plt.xlim(-0.5, 2.5)\n",
    "    plt.ylim(-0.5, 1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c221cc-4a31-448d-a446-95b3dfea921c",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522bae84-0013-46a9-af1d-2b6b9eec71cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5b – Week 5: Improving and evaluating our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71b03b4-2ae5-4dd1-ba49-e3d121582fa9",
   "metadata": {},
   "source": [
    "#### Overfitting Trees\n",
    "\n",
    "##### 11. Complete the function accuracy_by_treedepth. The function should return the median difference for both the testing and training data sets for each of the tree depths in depths.\n",
    "#### accuracy_by_treedepth should take the following arguments:\n",
    "\n",
    "> ##### features and targets (as in previous problems);\n",
    "> ##### features and targets (as in previous problems);\n",
    "\n",
    "##### Your function should return two lists (or arrays) containing the median_diff values for the predictions made on the training and test sets using the maximum tree depths given by the depths.\n",
    "\n",
    "##### For example, if depths is [3, 5, 7], then your function should return two lists of length 3. You can choose the size of the split between your testing and training data (if in doubt, 50:50 is fine).\n",
    "\n",
    "##### We've included code to plot the differences as a function of tree depths. You should take a moment to familiarise yourself with what each line is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5ab8d0-5d3e-45e0-99c6-23af37733098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# paste your get_features_targets function here\n",
    "def get_features_targets(data):\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "# paste your median_diff function here\n",
    "def median_diff(predicted, actual):\n",
    "    return [np.median(abs(predicted-actual))]\n",
    "\n",
    "                   \n",
    "# Complete the following function\n",
    "def accuracy_by_treedepth(features, targets, depths):\n",
    "    # split the data into testing and training sets\n",
    "    def split(features,targets):\n",
    "        split = features.shape[0]//2\n",
    "        return features[:split],features[split:],targets[:split],targets[split:]\n",
    "    train_features,test_features,train_targets,test_targets =split(features,targets)\n",
    "    # initialise arrays or lists to store the accuracies for the below loop\n",
    "    train=[]\n",
    "    test=[]\n",
    "\n",
    "    # loop through depths\n",
    "    for depth in depths:\n",
    "        # initialize model with the maximum depth. \n",
    "        dtr = DecisionTreeRegressor(max_depth=depth)\n",
    "\n",
    "        # train the model using the training set\n",
    "        # get the predictions for the training set and calculate their median_diff\n",
    "        dtr.fit(train_features, train_targets)\n",
    "        predictions = dtr.predict(train_features)\n",
    "        train=train+median_diff(train_targets, predictions)\n",
    "        # get the predictions for the testing set and calculate their median_diff\n",
    "        predictions = dtr.predict(test_features)\n",
    "        test=test+median_diff(test_targets, predictions)\n",
    "    # return the accuracies for the training and testing sets\n",
    "    return train,test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('sdss_galaxy_colors.npy')\n",
    "    features, targets = get_features_targets(data)\n",
    "\n",
    "    # Generate several depths to test\n",
    "    tree_depths = [i for i in range(1, 36, 2)]\n",
    "\n",
    "    # Call the function\n",
    "    train_med_diffs, test_med_diffs = accuracy_by_treedepth(features, targets, tree_depths)\n",
    "    print(\"Depth with lowest median difference : {}\".format(tree_depths[test_med_diffs.index(min(test_med_diffs))]))\n",
    "    \n",
    "    # Plot the results\n",
    "    train_plot = plt.plot(tree_depths, train_med_diffs, label='Training set')\n",
    "    test_plot = plt.plot(tree_depths, test_med_diffs, label='Validation set')\n",
    "    plt.xlabel(\"Maximum Tree Depth\")\n",
    "    plt.ylabel(\"Median of Differences\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efdb06e-46d8-4043-8dec-48a8cc84e68c",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72685cde-cc84-4ba8-975e-fdb074fec449",
   "metadata": {},
   "source": [
    "#### KFold Cross Validation\n",
    "\n",
    "##### 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3314436-0697-44d2-9b68-c4b76ddeba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# paste your get_features_targets function here\n",
    "def get_features_targets(data):\n",
    "    # complete this function\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "# paste your median_diff function here\n",
    "def median_diff(predicted, actual):\n",
    "    return [np.median(abs(predicted-actual))]\n",
    "\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_model(model, features, targets, k):\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # initialise a list to collect median_diffs for each iteration of the loop below\n",
    "    x=[]\n",
    "    for train_indices, test_indices in kf.split(features):\n",
    "        train_features, test_features = features[train_indices], features[test_indices]\n",
    "        train_targets, test_targets = targets[train_indices], targets[test_indices]\n",
    "    \n",
    "    # fit the model for the current set\n",
    "    model.fit(train_features, train_targets)\n",
    "    predictions = model.predict(test_features)\n",
    "    # predict using the model\n",
    "    # calculate the median_diff from predicted values and append to results array\n",
    "    x=x+median_diff(test_targets, predictions)\n",
    " \n",
    "    # return the list with your median difference values\n",
    "    return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('./sdss_galaxy_colors.npy')\n",
    "    features, targets = get_features_targets(data)\n",
    "\n",
    "    # initialize model with a maximum depth of 19\n",
    "    dtr = DecisionTreeRegressor(max_depth=19)\n",
    "\n",
    "    # call your cross validation function\n",
    "    diffs = cross_validate_model(dtr, features, targets, 10)\n",
    "\n",
    "    # Print the values\n",
    "    print('Differences: {}'.format(', '.join(['{:.3f}'.format(val) for val in diffs])))\n",
    "    print('Mean difference: {:.3f}'.format(np.mean(diffs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390df6b-d9c9-4819-af3d-10fd2b70c9f1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b7260f-8356-4ca6-aabc-ebac5fd1295f",
   "metadata": {},
   "source": [
    "#### KFold Cross Validated Predictions\n",
    "\n",
    "##### 13. Complete the function cross_validate_predictions. This is very similar to the previous question except instead of returning the med_diff accuracy measurements we would like to return a predicted value for each of the galaxies.\n",
    "\n",
    "##### The function takes the same 4 arguments as the previous question, i.e. model, feaures, targets and k.\n",
    "\n",
    "##### Your function should return a single variable. The returned variable should be a 1-D numpy array of length *m*, where *m*\n",
    "##### is the number of galaxies in our data set. You should make sure that you maintain the order of galaxies when giving your predictions, such that the first prediction in your array corresponds to the first galaxy in the features and targets arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b96eea-8380-4629-87dc-e85eb19a795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# paste your get_features_targets function here\n",
    "def get_features_targets(data):\n",
    "    # complete this function\n",
    "    x=[]\n",
    "    for n in range (len(data)):\n",
    "        x=x+[(data['u'][n] - data['g'][n],data['g'][n] - data['r'][n],data['r'][n] - data['i'][n],data['i'][n] - data['z'][n])]\n",
    "    return np.array(x),data['redshift']\n",
    "\n",
    "\n",
    "# paste your median_diff function here\n",
    "def median_diff(predicted, actual):\n",
    "    return np.median(abs(predicted-actual))\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_predictions(model, features, targets, k):\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "    # declare an array for predicted redshifts from each iteration\n",
    "    all_predictions = np.zeros_like(targets)\n",
    "\n",
    "    for train_indices, test_indices in kf.split(features):\n",
    "        # split the data into training and testing\n",
    "        train_features, test_features = features[train_indices], features[test_indices]\n",
    "        train_targets, test_targets = targets[train_indices], targets[test_indices]\n",
    "\n",
    "        # fit the model for the current set\n",
    "        model.fit(train_features, train_targets)\n",
    "        predictions = model.predict(test_features)\n",
    "        \n",
    "        # predict using the model\n",
    "        \n",
    "        # put the predicted values in the all_predictions array defined above\n",
    "        all_predictions[test_indices] = predictions\n",
    "    # return the predictions\n",
    "    return all_predictions    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('./sdss_galaxy_colors.npy')\n",
    "    features, targets = get_features_targets(data)\n",
    "\n",
    "    # initialize model\n",
    "    dtr = DecisionTreeRegressor(max_depth=19)\n",
    "\n",
    "    # call your cross validation function\n",
    "    predictions = cross_validate_predictions(dtr, features, targets, 10)\n",
    "\n",
    "    # calculate and print the rmsd as a sanity check\n",
    "    diffs = median_diff(predictions, targets)\n",
    "    print('Median difference: {:.3f}'.format(diffs))\n",
    "\n",
    "    # plot the results to see how well our model looks\n",
    "    plt.scatter(targets, predictions, s=0.4)\n",
    "    plt.xlim((0, targets.max()))\n",
    "    plt.ylim((0, predictions.max()))\n",
    "    plt.xlabel('Measured Redshift')\n",
    "    plt.ylabel('Predicted Redshift')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4bc5de-d10d-4672-9af2-e06dcdf47a9e",
   "metadata": {},
   "source": [
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a820a6-f336-4ea0-95ab-e3cd83469a88",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6a – Week 6: Exploring machine learning classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b57bb5-a8c6-4cfa-bc74-c8f00e396718",
   "metadata": {},
   "source": [
    "#### Splitting the train and test sets\n",
    "##### 14. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d516df1-896b-48bb-adf3-3d119b26fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def splitdata_train_test(data, fraction_training):\n",
    "    \n",
    "    # complete this function\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data)\n",
    "    split= int(len(data)*fraction_training)\n",
    "    return data[:split],data[split:]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "    # set the fraction of data which should be in the training set\n",
    "    fraction_training = 0.7\n",
    "\n",
    "    # split the data using your function\n",
    "    training, testing = splitdata_train_test(data, fraction_training)\n",
    "\n",
    "    # print the key values\n",
    "    print('Number data galaxies:', len(data))\n",
    "    print('Train fraction:', fraction_training)\n",
    "    print('Number of galaxies in training set:', len(training))\n",
    "    print('Number of galaxies in testing set:', len(testing))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f57751b-b06d-4ca7-bbcc-129fbb090924",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11926a26-a399-4def-a8b2-1d41a9817efd",
   "metadata": {},
   "source": [
    "#### Generating features and targets\n",
    "\n",
    "##### 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ef00a-132c-494c-8be2-18c15699ac20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_features_targets(data):\n",
    "    # complete the function by calculating the concentrations\n",
    "    targets = data['class']\n",
    "    features = np.empty(shape=(len(data), 13))\n",
    "    features[:, 0] = data['u-g']\n",
    "    features[:, 1] = data['g-r']\n",
    "    features[:, 2] = data['r-i']\n",
    "    features[:, 3] = data['i-z']\n",
    "    features[:, 4] = data['ecc']\n",
    "    features[:, 5] = data['m4_u']\n",
    "    features[:, 6] = data['m4_g']\n",
    "    features[:, 7] = data['m4_r']\n",
    "    features[:, 8] = data['m4_i']\n",
    "    features[:, 9] = data['m4_z']\n",
    "\n",
    "    # fill the remaining 3 columns with concentrations in the u, r and z filters\n",
    "    # concentration in u filter\n",
    "    features[:, 10] = data['petroR50_u']/data['petroR90_u']\n",
    "    # concentration in r filter\n",
    "    features[:, 11] = data['petroR50_r']/data['petroR90_r']\n",
    "    # concentration in z filter\n",
    "    features[:, 12] = data['petroR50_z']/data['petroR90_z']\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "    features, targets = generate_features_targets(data)\n",
    "\n",
    "    # Print the shape of each array to check the arrays are the correct dimensions. \n",
    "    print(\"Features shape:\", features.shape)\n",
    "    print(\"Targets shape:\", targets.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f5e250-54f2-4afd-80b8-38a38db2bd1b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9d5fbd-8e67-429c-81aa-5966b348ec27",
   "metadata": {},
   "source": [
    "#### Train the decision tree classifier\n",
    "##### 16. It is time to use the functions we wrote to split the data and generate the features, and then train a decision tree classifier.\n",
    "\n",
    "##### Your task is complete the *dtc_predict_actual* function by following the Python comments. The purpose of the function is to perform a held out validation and return the predicted and actual classes for later comparison.\n",
    "\n",
    "##### The function takes a single argument which is the full data set and should return two NumPy arrays containing the predicted and actual classes respectively.\n",
    "\n",
    "##### You will also need to copy your solutions from the previous two questions into the spaces allocated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c8cde8-d153-4556-ab37-0bdb4489ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# copy your splitdata_train_test function here\n",
    "def splitdata_train_test(data, fraction_training):\n",
    "    \n",
    "    # complete this function\n",
    "    np.random.seed(0)\n",
    "    np.random.shuffle(data)\n",
    "    split= int(len(data)*fraction_training)\n",
    "    return data[:split],data[split:]\n",
    "\n",
    "# copy your generate_features_targets function here\n",
    "def generate_features_targets(data):\n",
    "    \n",
    "    # complete the function by calculating the concentrations\n",
    "    targets = data['class']\n",
    "\n",
    "    features = np.empty(shape=(len(data), 13))\n",
    "    features[:, 0] = data['u-g']\n",
    "    features[:, 1] = data['g-r']\n",
    "    features[:, 2] = data['r-i']\n",
    "    features[:, 3] = data['i-z']\n",
    "    features[:, 4] = data['ecc']\n",
    "    features[:, 5] = data['m4_u']\n",
    "    features[:, 6] = data['m4_g']\n",
    "    features[:, 7] = data['m4_r']\n",
    "    features[:, 8] = data['m4_i']\n",
    "    features[:, 9] = data['m4_z']\n",
    "\n",
    "    # fill the remaining 3 columns with concentrations in the u, r and z filters\n",
    "    # concentration in u filter\n",
    "    features[:, 10] = data['petroR50_u']/data['petroR90_u']\n",
    "    # concentration in r filter\n",
    "    features[:, 11] = data['petroR50_r']/data['petroR90_r']\n",
    "    # concentration in z filter\n",
    "    features[:, 12] = data['petroR50_z']/data['petroR90_z']\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "\n",
    "\n",
    "# complete this function by splitting the data set and training a decision tree classifier\n",
    "def dtc_predict_actual(data):\n",
    "    \n",
    "    # split the data into training and testing sets using a training fraction of 0.7\n",
    "    training,testing=splitdata_train_test(data, 0.7)\n",
    "\n",
    "    # generate the feature and targets for the training and test sets\n",
    "    training_features,training_targets= generate_features_targets(training)\n",
    "    # i.e. train_features, train_targets, test_features, test_targets\n",
    "    testing_features,testing_targets= generate_features_targets(testing)\n",
    "    # instantiate a decision tree classifier\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    # train the classifier with the train_features and train_targets\n",
    "    dtc.fit(training_features, training_targets)\n",
    "    predictions = dtc.predict(testing_features)\n",
    "\n",
    "    # get predictions for the test_features\n",
    "\n",
    "    # return the predictions and the test_targets\n",
    "    return predictions,testing_targets\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data = np.load('galaxy_catalogue.npy')\n",
    "    \n",
    "    predicted_class, actual_class = dtc_predict_actual(data)\n",
    "\n",
    "    # Print some of the initial results\n",
    "    print(\"Some initial results...\\n   predicted,  actual\")\n",
    "    for i in range(10):\n",
    "        print(\"{}. {}, {}\".format(i, predicted_class[i], actual_class[i]))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf11319-e51e-4409-a6be-b00d602f613b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fc9d32-f11c-433f-ac63-391bc9221c0e",
   "metadata": {},
   "source": [
    "#### Accuracy in classification\n",
    "\n",
    "##### 17. Your task is to complete the calculate_accuracy function. The function should calculate the accuracy: the fraction of predictions that are correct (i.e. the model score):\n",
    "\n",
    "> #### Word\n",
    "\n",
    "##### The function takes two arguments;\n",
    "\n",
    "> ##### predicted: an array of the predicted class for each galaxy.\n",
    "> ##### actual: an array of the actual class for each galaxy.\n",
    "\n",
    "##### The return value should be a float (between 0 and 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd171b1-d4cd-497b-90e2-0797216ccac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from support_functions import plot_confusion_matrix, generate_features_targets\n",
    "\n",
    "\n",
    "# Implement the following function\n",
    "def calculate_accuracy(predicted, actual):\n",
    "    x=0\n",
    "    for n in range (len(predicted)):\n",
    "        if predicted[n]==actual[n]:\n",
    "            x=x+1\n",
    "    accuracy=x/len(predicted)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "    # split the data\n",
    "    features, targets = generate_features_targets(data)\n",
    "\n",
    "    # train the model to get predicted and actual classes\n",
    "    dtc = DecisionTreeClassifier()\n",
    "    predicted = cross_val_predict(dtc, features, targets, cv=10)\n",
    "\n",
    "    # calculate the model score using your function\n",
    "    model_score = calculate_accuracy(predicted, targets)\n",
    "    print(\"Our accuracy score:\", model_score)\n",
    "\n",
    "    # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "    class_labels = list(set(targets))\n",
    "    model_cm = confusion_matrix(y_true=targets, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "    # Plot the confusion matrix using the provided functions.\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c34d294-23ad-45d1-9949-0e3248417702",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263a3b5-425a-4af9-a0b4-3b18bbc80b05",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "##### 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859daa05-3963-4292-bb09-61b5122b0102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from support_functions import generate_features_targets, plot_confusion_matrix, calculate_accuracy\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# complete this function to get predictions from a random forest classifier\n",
    "def rf_predict_actual(data, n_estimators):\n",
    "    \n",
    "    # generate the features and targets\n",
    "    targets = data['class']\n",
    "    features = np.empty(shape=(len(data), 13))\n",
    "    features[:, 0] = data['u-g']\n",
    "    features[:, 1] = data['g-r']\n",
    "    features[:, 2] = data['r-i']\n",
    "    features[:, 3] = data['i-z']\n",
    "    features[:, 4] = data['ecc']\n",
    "    features[:, 5] = data['m4_u']\n",
    "    features[:, 6] = data['m4_g']\n",
    "    features[:, 7] = data['m4_r']\n",
    "    features[:, 8] = data['m4_i']\n",
    "    features[:, 9] = data['m4_z']\n",
    "    features[:, 10] = data['petroR50_u']/data['petroR90_u']\n",
    "    features[:, 11] = data['petroR50_r']/data['petroR90_r']\n",
    "    features[:, 12] = data['petroR50_z']/data['petroR90_z']\n",
    "    # instantiate a random forest classifier using n estimators\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators)\n",
    "    # get predictions using 10-fold cross validation with cross_val_predict\n",
    "    a=cross_val_predict(rfc, features, targets,cv=10)\n",
    "    # return the predictions and their actual classes\n",
    "    return a,targets\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = np.load('galaxy_catalogue.npy')\n",
    "\n",
    "    # get the predicted and actual classes\n",
    "    number_estimators = 50              # Number of trees\n",
    "    predicted, actual = rf_predict_actual(data, number_estimators)\n",
    "\n",
    "    # calculate the model score using your function\n",
    "    accuracy = calculate_accuracy(predicted, actual)\n",
    "    print(\"Accuracy score:\", accuracy)\n",
    "\n",
    "    # calculate the models confusion matrix using sklearns confusion_matrix function\n",
    "    class_labels = list(set(actual))\n",
    "    model_cm = confusion_matrix(y_true=actual, y_pred=predicted, labels=class_labels)\n",
    "\n",
    "    # plot the confusion matrix using the provided functions.\n",
    "    plt.figure()\n",
    "    plot_confusion_matrix(model_cm, classes=class_labels, normalize=False)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7a5395-5a96-4495-8e32-d8607132bc42",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
